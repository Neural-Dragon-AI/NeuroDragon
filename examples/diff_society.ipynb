{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 10, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 20, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 30, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 40, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 50, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 60, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 70, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 80, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 90, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 100, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 110, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 120, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 130, Loss: 0.0, Reward: 0.0, Output: age\n",
      "Episode: 140, Loss: 0.0, Reward: 0.0, Output: age\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mlog(attention_weights[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]) \u001b[39m*\u001b[39m reward\n\u001b[0;32m     67\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 68\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     71\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode: \u001b[39m\u001b[39m{\u001b[39;00mepisode\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m, Reward: \u001b[39m\u001b[39m{\u001b[39;00mreward\u001b[39m}\u001b[39;00m\u001b[39m, Output: \u001b[39m\u001b[39m{\u001b[39;00moutput_tokens\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    412\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class AttentionMemory(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionMemory, self).__init__()\n",
    "        self.attention = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_embed):\n",
    "        attention_weights = torch.softmax(self.attention(input_embed), dim=1)\n",
    "        return attention_weights\n",
    "\n",
    "class Chatbot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Chatbot, self).__init__()\n",
    "        self.embedder = AutoModel.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.attention_memory = AttentionMemory()\n",
    "\n",
    "    def forward(self, input_text, memories):\n",
    "        input_tokens = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_embed = self.embedder(**input_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        memory_embeddings = []\n",
    "        for memory in memories:\n",
    "            memory_tokens = self.tokenizer(memory, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            memory_embed = self.embedder(**memory_tokens).last_hidden_state.mean(dim=1)\n",
    "            memory_embeddings.append(memory_embed)\n",
    "\n",
    "        memory_embeddings = torch.stack(memory_embeddings, dim=1).squeeze(0)\n",
    "        attention_weights = self.attention_memory(input_embed)\n",
    "        weighted_memory = torch.matmul(attention_weights, memory_embeddings)\n",
    "\n",
    "        return weighted_memory, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "model = Chatbot()\n",
    "optimizer = torch.optim.Adam(model.attention_memory.parameters(), lr=0.01)\n",
    "\n",
    "input_text = \"What is the capital of France?\"\n",
    "memories = [\"The capital of France is Paris.\", \"The capital of Germany is Berlin.\"]\n",
    "target_output = \"The capital of France is Paris.\"\n",
    "\n",
    "num_episodes = 1000\n",
    "gamma = 0.99\n",
    "\n",
    "def reward_function(output_tokens, target_output):\n",
    "    return 1.0 if output_tokens.strip() == target_output.strip() else 0.0\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_episodes = 1000\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    optimizer.zero_grad()\n",
    "    output, attention_weights = model(input_text, memories)\n",
    "    \n",
    "    output_tokens = model.tokenizer.decode(output.argmax(dim=1).tolist(), skip_special_tokens=True)\n",
    "    reward = reward_function(output_tokens, target_output)\n",
    "\n",
    "    loss = -torch.log(attention_weights[0, 0]) * reward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode: {episode}, Loss: {loss.item()}, Reward: {reward}, Output: {output_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " think you derailed let's start from scratch:\n",
    "\n",
    "the model takes as input the question and uses this class to transform it in a vector\n",
    "\n",
    "class OpenAiEmbedder:\n",
    "    def get_embedding_size(self):\n",
    "        return 1536\n",
    "    def embed(self, data, embed_mark = True, verbose = False):\n",
    "        try:\n",
    "            if embed_mark is False and type(data) is dict and \"content\" in data:\n",
    "                print(\"Embedding without mark\", data[\"content\"])\n",
    "                out = openai.Embedding.create(input=data[\"content\"], engine='text-embedding-ada-002')\n",
    "            else:\n",
    "                if verbose is True:\n",
    "                    print(\"Embedding without preprocessing the input\", data)\n",
    "                out = openai.Embedding.create(input=str(data), engine='text-embedding-ada-002')\n",
    "        except:\n",
    "            raise ValueError(\"The data  is not valid\")\n",
    "        return out.data[0].embedding\n",
    "    def embed_list(self,data):\n",
    "        #use the batched version of the API by giving a list as input\n",
    "        #che that is listo of strings\n",
    "        if type(data) is not list:\n",
    "            raise ValueError(\"The data  is not valid\")\n",
    "        out = openai.Embedding.create(input=data, engine='text-embedding-ada-002')\n",
    "\n",
    "then it uses the same embedding class to his memory and uses the three embedded vectors to with a self attention mechanism to choose which of the two memory to choose. If the memory chosen is the good memory reward of +1 otherwise reward of 0, very very simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommaso\\AppData\\Local\\Temp\\ipykernel_28272\\1922798265.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  similarity = cosine_similarity(chosen_memory, torch.tensor(good_memory_embedding))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Loss: 0.6417206525802612, Reward: 0.926105797290802\n",
      "Episode: 50, Loss: 0.0059396191500127316, Reward: 0.9999890327453613\n",
      "Episode: 100, Loss: 0.003237340599298477, Reward: 0.999995231628418\n",
      "Episode: 150, Loss: 0.002143227495253086, Reward: 0.9999969005584717\n",
      "Episode: 200, Loss: 0.001533786067739129, Reward: 0.9999974966049194\n",
      "Episode: 250, Loss: 0.0011595649411901832, Reward: 0.9999997615814209\n",
      "Episode: 300, Loss: 0.0009122475748881698, Reward: 0.9999997615814209\n",
      "Episode: 350, Loss: 0.0007396688451990485, Reward: 0.9999997019767761\n",
      "Episode: 400, Loss: 0.0006137584568932652, Reward: 0.9999998807907104\n",
      "Episode: 450, Loss: 0.0005188737995922565, Reward: 1.0\n",
      "Final attention weights: tensor([[[9.9955e-01],\n",
      "         [4.4558e-04]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "class OpenAiEmbedder:\n",
    "    def get_embedding_size(self):\n",
    "        return 1536\n",
    "    def embed(self, data, embed_mark = True, verbose = False):\n",
    "        try:\n",
    "            if embed_mark is False and type(data) is dict and \"content\" in data:\n",
    "                print(\"Embedding without mark\", data[\"content\"])\n",
    "                out = openai.Embedding.create(input=data[\"content\"], engine='text-embedding-ada-002')\n",
    "            else:\n",
    "                if verbose is True:\n",
    "                    print(\"Embedding without preprocessing the input\", data)\n",
    "                out = openai.Embedding.create(input=str(data), engine='text-embedding-ada-002')\n",
    "        except:\n",
    "            raise ValueError(\"The data  is not valid\")\n",
    "        return out.data[0].embedding\n",
    "    def embed_list(self,data):\n",
    "        #use the batched version of the API by giving a list as input\n",
    "        #che that is listo of strings\n",
    "        if type(data) is not list:\n",
    "            raise ValueError(\"The data  is not valid\")\n",
    "        out = openai.Embedding.create(input=data, engine='text-embedding-ada-002')\n",
    "\n",
    "class SimpleAttentionModel(nn.Module):\n",
    "    def __init__(self, embedder):\n",
    "        super(SimpleAttentionModel, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.attention = nn.Linear(embedder.get_embedding_size(), 1)\n",
    "\n",
    "    def forward(self, question, memories):\n",
    "        question_embedding = torch.tensor(self.embedder.embed(question)).unsqueeze(0)\n",
    "        memory_embeddings = torch.tensor([self.embedder.embed(memory) for memory in memories]).unsqueeze(0)\n",
    "\n",
    "        attention_logits = self.attention(memory_embeddings)\n",
    "        attention_weights = torch.softmax(attention_logits, dim=1)\n",
    "\n",
    "        chosen_memory = torch.sum(attention_weights * memory_embeddings, dim=1).squeeze()\n",
    "        return chosen_memory, attention_weights\n",
    "\n",
    "# ... (Same as before: OpenAiEmbedder and SimpleAttentionModel classes) ...\n",
    "\n",
    "def reward_function(chosen_memory, good_memory_embedding):\n",
    "    cosine_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "    similarity = cosine_similarity(chosen_memory, torch.tensor(good_memory_embedding))\n",
    "    return similarity.item()\n",
    "\n",
    "# Example usage\n",
    "openai_embedder = OpenAiEmbedder()\n",
    "model = SimpleAttentionModel(openai_embedder)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "question = \"What is the capital of France?\"\n",
    "memories = [\"The capital of France is Paris.\", \"An irrelevant memory.\"]\n",
    "good_memory = memories[0]\n",
    "\n",
    "num_episodes = 500\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    chosen_memory, attention_weights = model(question, memories)\n",
    "    reward = reward_function(chosen_memory, torch.tensor(openai_embedder.embed(good_memory)))\n",
    "\n",
    "    loss = -torch.log(attention_weights[0, 0]) * reward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode: {episode}, Loss: {loss.item()}, Reward: {reward}\")\n",
    "\n",
    "print(\"Final attention weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommaso\\AppData\\Local\\Temp\\ipykernel_28272\\735125926.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  similarity = cosine_similarity(chosen_memory, torch.tensor(good_memory_embedding))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Loss: 1.0000296831130981, Reward: 0.94023197889328\n",
      "Episode: 50, Loss: -0.0, Reward: 1.0000001192092896\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The data  is not valid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m, in \u001b[0;36mOpenAiEmbedder.embed\u001b[1;34m(self, data, embed_mark, verbose)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEmbedding without preprocessing the input\u001b[39m\u001b[39m\"\u001b[39m, data)\n\u001b[1;32m---> 18\u001b[0m         out \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mcreate(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m(data), engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtext-embedding-ada-002\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m     \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[39m# This is only for the default case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    138\u001b[0m (\n\u001b[0;32m    139\u001b[0m     deployment_id,\n\u001b[0;32m    140\u001b[0m     engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m )\n\u001b[1;32m--> 153\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m     url,\n\u001b[0;32m    156\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m     request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[0;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[0;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    517\u001b[0m         method,\n\u001b[0;32m    518\u001b[0m         abs_url,\n\u001b[0;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[0;32m    525\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1375\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episodes):\n\u001b[0;32m     44\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 46\u001b[0m     chosen_memory, attention_weights \u001b[39m=\u001b[39m model(question, memories)\n\u001b[0;32m     47\u001b[0m     reward \u001b[39m=\u001b[39m reward_function(chosen_memory, torch\u001b[39m.\u001b[39mtensor(openai_embedder\u001b[39m.\u001b[39membed(good_memory)))\n\u001b[0;32m     49\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mlog(attention_weights[\u001b[39m0\u001b[39m]) \u001b[39m*\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\Tommaso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m, in \u001b[0;36mSimpleAttentionModel.forward\u001b[1;34m(self, question, memories)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, question, memories):\n\u001b[0;32m     15\u001b[0m     question_embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder\u001b[39m.\u001b[39membed(question))\n\u001b[1;32m---> 16\u001b[0m     memory_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder\u001b[39m.\u001b[39membed(memory) \u001b[39mfor\u001b[39;00m memory \u001b[39min\u001b[39;00m memories])\n\u001b[0;32m     18\u001b[0m     conv1_question \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(question_embedding)\n\u001b[0;32m     19\u001b[0m     conv1_memories \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(memory_embeddings)\n",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, question, memories):\n\u001b[0;32m     15\u001b[0m     question_embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder\u001b[39m.\u001b[39membed(question))\n\u001b[1;32m---> 16\u001b[0m     memory_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedder\u001b[39m.\u001b[39;49membed(memory) \u001b[39mfor\u001b[39;00m memory \u001b[39min\u001b[39;00m memories])\n\u001b[0;32m     18\u001b[0m     conv1_question \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(question_embedding)\n\u001b[0;32m     19\u001b[0m     conv1_memories \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(memory_embeddings)\n",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m, in \u001b[0;36mOpenAiEmbedder.embed\u001b[1;34m(self, data, embed_mark, verbose)\u001b[0m\n\u001b[0;32m     18\u001b[0m         out \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mEmbedding\u001b[39m.\u001b[39mcreate(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(data), engine\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtext-embedding-ada-002\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe data  is not valid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mdata[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39membedding\n",
      "\u001b[1;31mValueError\u001b[0m: The data  is not valid"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import openai\n",
    "\n",
    "\n",
    "class SimpleAttentionModel(nn.Module):\n",
    "    def __init__(self, embedder):\n",
    "        super(SimpleAttentionModel, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.embedding_size = embedder.get_embedding_size()\n",
    "        self.conv1 = nn.Linear(self.embedding_size, self.embedding_size)\n",
    "\n",
    "    def forward(self, question, memories):\n",
    "        question_embedding = torch.tensor(self.embedder.embed(question))\n",
    "        memory_embeddings = torch.tensor([self.embedder.embed(memory) for memory in memories])\n",
    "\n",
    "        conv1_question = self.conv1(question_embedding)\n",
    "        conv1_memories = self.conv1(memory_embeddings)\n",
    "\n",
    "        dot_products = torch.matmul(conv1_memories, conv1_question)\n",
    "\n",
    "        attention_weights = torch.softmax(dot_products, dim=-1)\n",
    "\n",
    "        chosen_memory = torch.sum(attention_weights.unsqueeze(-1) * memory_embeddings, dim=0)\n",
    "        return chosen_memory, attention_weights\n",
    "\n",
    "def reward_function(chosen_memory, good_memory_embedding):\n",
    "    cosine_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "    similarity = cosine_similarity(chosen_memory, torch.tensor(good_memory_embedding))\n",
    "    return similarity.item()\n",
    "\n",
    "openai_embedder = OpenAiEmbedder()\n",
    "model = SimpleAttentionModel(openai_embedder)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "question = \"What is the capital of France?\"\n",
    "memories = [\"The capital of France is Paris.\", \"An irrelevant memory.\", \"the capital of italy is rome\"]\n",
    "good_memory = memories[0]\n",
    "\n",
    "num_episodes = 500\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    chosen_memory, attention_weights = model(question, memories)\n",
    "    reward = reward_function(chosen_memory, torch.tensor(openai_embedder.embed(good_memory)))\n",
    "\n",
    "    loss = -torch.log(attention_weights[0]) * reward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode: {episode}, Loss: {loss.item()}, Reward: {reward}\")\n",
    "\n",
    "print(\"Final attention weights:\", attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
