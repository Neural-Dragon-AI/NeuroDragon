{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First attempt at writing a memory module for ChatGpt Api\n",
    "\n",
    "* Simply track a history of messages until capacity. Done\n",
    "* Fifo Queue - Done\n",
    "* Vector Storage, memory is created by filling the context only with messages from the storage, \n",
    "    * either in Q/A pairs or at message level - Done \n",
    "    * either ordered in terms of similarity DONE\n",
    "    * or chronological -  TODO\n",
    "    * summarized in a single message or less messages the {user} token are still wasteful (should check if they use only one for that) --> 6 tokens extra per message\n",
    "* Fifo Queue with Outs into VectorStorage, half of the memory is filled with samples from the vector storage / half from retrieval.  TODO\n",
    "* Compressed Fifo Queue, a compressor thread creates a minified version of the original memory thread creating an information-bottleneck TO REFACTOR\n",
    "* Compressed Fifo Retrieval, a compressor thread creates a minified version of the original memory, a attention thread selects compressed messages that are releavant for the answer and use them as seed for retrieval TODO\n",
    "    * potentially create the retrieval prompt with a recaller thread TODO\n",
    "\n",
    "Lacking of examples sorry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "import openai\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAiEmbedder:\n",
    "    def get_embedding_size(self):\n",
    "        return 1536\n",
    "    def embed(self, data, embed_mark = True, verbose = False):\n",
    "        try:\n",
    "            if embed_mark is False and type(data) is dict and \"content\" in data:\n",
    "                print(\"Embedding without mark\", data[\"content\"])\n",
    "                out = openai.Embedding.create(input=data[\"content\"], engine='text-embedding-ada-002')\n",
    "            else:\n",
    "                if verbose is True:\n",
    "                    print(\"Embedding without preprocessing the input\", data)\n",
    "                out = openai.Embedding.create(input=str(data), engine='text-embedding-ada-002')\n",
    "        except:\n",
    "            raise ValueError(\"The data  is not valid\")\n",
    "        return out.data[0].embedding\n",
    "    def embed_list(self,data):\n",
    "        #use the batched version of the API by giving a list as input\n",
    "        #che that is listo of strings\n",
    "        if type(data) is not list:\n",
    "            raise ValueError(\"The data  is not valid\")\n",
    "        out = openai.Embedding.create(input=data, engine='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_system(system_prompt):\n",
    "    return {\"role\": \"system\", \"content\": system_prompt}\n",
    "def mark_answer(answer):\n",
    "    return {\"role\": \"assistant\", \"content\": answer}\n",
    "def mark_question(question):\n",
    "    return {\"role\": \"user\", \"content\": question}\n",
    "def check_dict(message_dict):\n",
    "        if type(message_dict) is list and len(message_dict) == 1 and type(message_dict[0]) is dict:\n",
    "            message_dict = message_dict[0]\n",
    "        elif type(message_dict) is not dict:\n",
    "            raise Exception(\"The message_dict should be a dictionary or a [dictionary] instead it is \", message_dict, type(message_dict))  \n",
    "        return message_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class MemoryIndex:\n",
    "    \"\"\" this class is a wrapper for a faiss index, it contains information about the format of the index the faiss index itself\"\"\"\n",
    "    def __init__(self, index = None,values = None, embeddings = None, name='memory_index', save_path = None, load= False):\n",
    "        self.name = name\n",
    "        self.embedder = OpenAiEmbedder()\n",
    "        self.save_path = save_path\n",
    "        # with load been through we search for a pickle file with the same name of the index\n",
    "        if load is True:\n",
    "            self.load()\n",
    "        else:\n",
    "            self.init_index(index,values,embeddings)\n",
    "\n",
    "\n",
    "    def init_index(self,index,values,embeddings):\n",
    "        #fist case is when we create a new index from scratch\n",
    "        if index is None and values is None and embeddings is None :\n",
    "            print(\"Creating a new index\")\n",
    "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
    "            self.values = []\n",
    "        #second case is where we create the index from a list of embeddings\n",
    "        elif index is None and values is not None and embeddings is not None and len(values) == len(embeddings):\n",
    "            print(\"Creating a new index from a list of embeddings and values\")\n",
    "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
    "            for embedding,value in zip(embeddings,values):\n",
    "                self.add_to_index_embedding(value, embedding) \n",
    "        #third case is where we create the index from a faiss index and values list  \n",
    "        elif isinstance(index, faiss.Index) and index.d == self.embedder.get_embedding_size() and type(values) == list and len(values) == index.ntotal:\n",
    "            print(\"Creating a new index from a faiss index and values list\")\n",
    "            self.index = index\n",
    "            self.values = values\n",
    "        #fourth case is where we create an index from a list of values, the values are embedded and the index is created\n",
    "        elif index is None and values is not None and embeddings is None:\n",
    "            print(\"Creating a new index from a list of values\")\n",
    "            self.index = faiss.IndexFlatIP(self.embedder.get_embedding_size())\n",
    "            for value in values:\n",
    "                self.add_to_index(value)\n",
    "        else:\n",
    "            raise ValueError(\"The index is not a valid faiss index or the embedding dimension is not correct\")\n",
    "\n",
    "    def add_to_index(self,value, verbose = False):\n",
    "        \"\"\"index a message in the faiss index, the message is embedded and the id is saved in the values list\n",
    "        \"\"\"\n",
    "        if value not in self.values:\n",
    "            try:\n",
    "                embedding = self.embedder.embed(value)\n",
    "                if verbose:\n",
    "                    display(Markdown(\"The value {value} was embedded\".format(value = value))) \n",
    "            except:\n",
    "                raise ValueError(\"The message cant be embedded\", value)\n",
    "        \n",
    "            self.index.add(np.array([embedding]).astype(np.float32))\n",
    "            self.values.append(value)\n",
    "        else:\n",
    "            if verbose:\n",
    "                display(Markdown(\"The value {value} was already in the index\".format(value = value)))\n",
    "\n",
    "    def add_to_index_embedding(self, value, embedding, verbose = False):\n",
    "        \"\"\"index a message in the faiss index, the message is embedded and the id is saved in the values list\n",
    "        \"\"\"\n",
    "        #check that the embedding is of the correct size and type, the type can be\n",
    "        # list of floats, numpy array of floats, string of a list of floats\n",
    "        # if list of floats convert to numpy array \n",
    "        # if string convert to list of floats using eval and then to numpy array\n",
    "        if type(embedding) is list:\n",
    "            embedding = np.array([embedding])\n",
    "        elif type(embedding) is str:\n",
    "            embedding = eval(embedding)\n",
    "            embedding = np.array([embedding]).astype(np.float32)\n",
    "        elif type(embedding) is not np.ndarray:\n",
    "            raise ValueError(\"The embedding is not a valid type\")\n",
    "        if value not in self.values:\n",
    "            self.index.add(embedding)\n",
    "            self.values.append(value)\n",
    "        else:\n",
    "            if verbose:\n",
    "                display(Markdown(\"The value {value} was already in the index\".format(value = value)))\n",
    "\n",
    "    def faiss_query(self, key, k = 10):\n",
    "        # Embed the data\n",
    "        embedding = self.embedder.embed(key)\n",
    "        if k > len(self.values):\n",
    "            k = len(self.values)\n",
    "        # Query the Faiss index for the top-K most similar values\n",
    "        D, I = self.index.search(np.array([embedding]).astype(np.float32), k)\n",
    "        values = [self.values[i] for i in I[0]]\n",
    "            \n",
    "        return values\n",
    "    def save(self, path=None):\n",
    "        \"\"\"saves the index and values to a pickle file\"\"\"\n",
    "        if path is None and self.save_path is None:\n",
    "            path = self.name + \".pkl\"\n",
    "        elif path is None and self.save_path is not None:\n",
    "            if self.save_path.endswith(\"/\"):\n",
    "                path = self.save_path + self.name + \".pkl\"\n",
    "            else:\n",
    "                path = self.save_path + \"/\" + self.name + \".pkl\"\n",
    "        print(\"Saving the index to \", path)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({'index': self.index, 'values': self.values}, f)\n",
    "\n",
    "    def load(self, path=None):\n",
    "        \"\"\"loads the index and values from a pickle file\"\"\"\n",
    "        if path is None and self.save_path is None:\n",
    "            path = self.name + \".pkl\"\n",
    "        elif path is None and self.save_path is not None:\n",
    "            if self.save_path.endswith(\"/\"):\n",
    "                path = self.save_path + self.name + \".pkl\"\n",
    "            else:\n",
    "                path = self.save_path + \"/\" + self.name + \".pkl\"\n",
    "\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.index = data['index']\n",
    "            self.values = data['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "\n",
    "class PandaIndex(MemoryIndex):\n",
    "    def __init__(self, pandaframe, columns=None, name='panda_index', save_path=None, in_place=True, embeddings_col=None):\n",
    "        self.columns = columns\n",
    "        self.values = []\n",
    "\n",
    "        # Load or copy pandaframe, and set self.name, self.columns\n",
    "        if type(pandaframe) == str and pandaframe.endswith(\".csv\") and os.path.isfile(pandaframe):\n",
    "            try:\n",
    "                pandaframe = pd.read_csv(pandaframe)\n",
    "            except:\n",
    "                raise ValueError(\"The CSV file is not valid\")\n",
    "            self.name = pandaframe.split(\"/\")[-1].split(\".\")[0]\n",
    "            self.columns = \"values\"\n",
    "        elif type(pandaframe) == pd.core.frame.DataFrame and columns is not None:\n",
    "            if not in_place:\n",
    "                pandaframe = copy.deepcopy(pandaframe)\n",
    "        else:\n",
    "            raise ValueError(\"The pandaframe is not a valid pandas dataframe or the columns are not valid or the path is not valid\")\n",
    "\n",
    "        values, embeddings = self.extract_values_and_embeddings(pandaframe, embeddings_col)\n",
    "        super().__init__(values=values, embeddings=embeddings, name=name, save_path=save_path)\n",
    "\n",
    "    def extract_values_and_embeddings(self, pandaframe, embeddings_col):\n",
    "        if type(self.columns) == list and len(self.columns) > 1:\n",
    "            pandaframe[\"values\"] = pandaframe[self.columns].apply(lambda x: ' '.join(x), axis=1)\n",
    "            self.columns = \"values\"\n",
    "        elif type(self.columns) == list and len(self.columns) == 1:\n",
    "            self.columns = self.columns[0]\n",
    "            pandaframe[\"values\"] = pandaframe[self.columns]\n",
    "            self.columns = \"values\"\n",
    "        elif type(self.columns) != str:\n",
    "            raise ValueError(\"The columns are not valid\")\n",
    "\n",
    "        values = []\n",
    "        embeddings = []\n",
    "\n",
    "        for _, row in pandaframe.iterrows():\n",
    "            value = row[\"values\"]\n",
    "            values.append(value)\n",
    "\n",
    "            if embeddings_col is not None:\n",
    "                embedding = row[embeddings_col]\n",
    "                embeddings.append(embedding)\n",
    "\n",
    "        return values, embeddings if embeddings_col is not None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "class MemoryThread:\n",
    "    \"\"\"this class is used to keep track of the memory thread and the total number of tokens all memories should subclass this class\n",
    "    if max_memory is None it has no limit to the number of tokens that can be stored in the memory thread \"\"\"\n",
    "    def __init__(self,name= 'memory',max_memory= None):\n",
    "        self.name = name\n",
    "        self.max_memory = max_memory\n",
    "        self.memory_thread = []\n",
    "        self.total_tokens = 0\n",
    "        self.tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "    def __getitem__(self, idx):\n",
    "        return self.memory_thread[idx]    \n",
    "\n",
    "        \n",
    "\n",
    "    def get_message_tokens(self, message_dict):\n",
    "        message = message_dict[\"content\"]\n",
    "        return len(self.tokenizer.encode(message))+6 # +6 for the role token\n",
    "\n",
    "    def remove_message(self, message_dict=  None , idx = None):\n",
    "        # if idx search in the memory_thread the latest message that matches the message_dict an\n",
    "        #remove it from the memory_thread otherwise remove the message at the index idx\n",
    "        # update the total number of tokens\n",
    "        # return a boolean that indicates if the message was found and removed\n",
    "        if message_dict is None and idx is None:\n",
    "            raise Exception(\"You need to provide either a message_dict or an idx\")\n",
    "        \n",
    "        if idx is None:\n",
    "            message_dict = check_dict(message_dict)\n",
    "            search_results = self.find_message(message_dict)\n",
    "            if search_results is not None:\n",
    "                idx = search_results[-1][\"idx\"]\n",
    "                message = search_results[-1][\"message\"]\n",
    "                self.memory_thread.pop(idx)\n",
    "                self.total_tokens -= self.get_message_tokens(message)\n",
    "            else:   \n",
    "                raise Exception(\"The message was not found in the memory thread\")\n",
    "        else:\n",
    "            if idx < len(self.memory_thread):\n",
    "                message = self.memory_thread.pop(idx)\n",
    "                self.total_tokens -= self.get_message_tokens(message)\n",
    "            else:  \n",
    "                raise Exception(\"The index was out bound\")\n",
    "    \n",
    "    def add_message(self,message_dict: dict):\n",
    "        # message_dict = {\"role\": role, \"content\": content}\n",
    "        #chek that the message_dict is a dictionary or a list of dictionaries \n",
    "        message_tokens = self.get_message_tokens(message_dict)\n",
    "        \n",
    "        if  self.max_memory is None or self.total_tokens + message_tokens <= self.max_memory:\n",
    "            #add the message_dict to the memory_thread\n",
    "            # update the total number of tokens\n",
    "            self.memory_thread.append(message_dict)\n",
    "            self.total_tokens += message_tokens\n",
    "            return True    \n",
    "        else :\n",
    "            display(Markdown(\"The memory thread is full, the last message was not added\"))\n",
    "            return False\n",
    "\n",
    "    \n",
    "                    \n",
    "    def find_message(self,message_dict: dict, last=False):\n",
    "        # search the memory_thread from start_idx to the end of the memory_thread for all the messages that match the message_dict\n",
    "        # return a seach_results dictionary with the following structure [{\"message\": message, \"idx\": idx}]\n",
    "        # if last is True return only the last message that matches the message_dict\n",
    "        search_results = []\n",
    "        message_dict = check_dict(message_dict)\n",
    "\n",
    "        print(\"the message dict is \", message_dict, type(message_dict))\n",
    "        for idx, message in enumerate(self.memory_thread):\n",
    "            print(\"the index is \", idx, type(idx))\n",
    "            print(\"the message is \", message, type(message))\n",
    "            if message[\"role\"] == message_dict[\"role\"] and message[\"content\"] == message_dict[\"content\"]:\n",
    "                search_results.append({\"message\": message, \"idx\": idx})\n",
    "        if last and len(search_results) > 0:\n",
    "            return [search_results[-1]]\n",
    "        elif len(search_results) > 0:\n",
    "            return search_results\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def length(self):\n",
    "        #return the length of the memory_thread\n",
    "        return len(self.memory_thread)\n",
    "    \n",
    "    def slice_tokens(self, start_idx= 0, end_idx = None ):\n",
    "        #compute the tokens from start_idx to end_idx\n",
    "        # default behavior is to compute the tokens of the whole memory_thread\n",
    "        tokens = 0\n",
    "        if end_idx is None:\n",
    "            end_idx = len(self.memory_thread)\n",
    "        try:\n",
    "            for message in self.memory_thread[start_idx:end_idx]:\n",
    "                tokens += self.get_message_tokens(message)\n",
    "            return tokens\n",
    "        except:\n",
    "            ValueError (\"The slice is not valid\")\n",
    "        \n",
    "    def get_message(self, idx: int ):\n",
    "        return self.memory_thread[idx]\n",
    "    \n",
    "    def get_thread(self):\n",
    "        return self.memory_thread\n",
    "    \n",
    "    def slice(self,start,end):\n",
    "        #return the memory_thread slice from start_idx to end_idx\n",
    "        # default behavior is to return the whole memory_thread\n",
    "        try:\n",
    "            return self.memory_thread[start:end]\n",
    "        except:\n",
    "            ValueError (\"The slice is not valid\")\n",
    "\n",
    "    def print(self):\n",
    "        # detailed output of the memory_thread using markdown\n",
    "        \n",
    "        display(Markdown(\"## Memory Thread\"))\n",
    "        display(Markdown(\"#### Total Tokens: \"+str(self.total_tokens)))\n",
    "        display(Markdown(\"#### Max Tokens: \"+str(self.max_memory)))\n",
    "        display(Markdown(\"#### Number of Messages: \"+str(len(self.memory_thread))))\n",
    "        display(Markdown(\"#### Messages:\"))\n",
    "        for message in self.memory_thread:\n",
    "            display(Markdown(\"#### \"+message[\"role\"]+\": \"+message[\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "class FifoMemory(MemoryThread):\n",
    "    \"\"\"FIFO Memory Thread, the oldest messages are removed first when reaching the max_memory limit, the memory is defined in terms of tokens, \n",
    "    outs are passe to the longterm_memory, \n",
    "    lucid_memory is a redundant memory that stores all the messages\n",
    "    \"\"\"\n",
    "    def __init__(self, name= 'fifo_memory', max_memory = None, longterm_thread = None):\n",
    "        \n",
    "        super().__init__(name= name , max_memory= max_memory)\n",
    "        self.lucid_thread = MemoryThread(name = 'lucid_memory',max_memory = None)\n",
    "        if longterm_thread is None:\n",
    "            self.longterm_thread = MemoryThread(name ='longterm_memory',max_memory = None)\n",
    "        else:\n",
    "            self.longterm_thread = longterm_thread\n",
    "        # create an alias for the memory_thread to make the code more readable\n",
    "        self.fifo_thread = self.memory_thread\n",
    "        \n",
    "        \n",
    "    def to_longterm(self, idx):\n",
    "        #move the message at the index idx to the longterm_memory\n",
    "        display(Markdown(\"The memory thread is full, the oldest message with index {} was moved to the longterm memory\".format(idx)))\n",
    "        message = copy.deepcopy(self.memory_thread[idx])\n",
    "        print(\"preso il messagio e provo a ad aggiungerlo al longterm\", message)\n",
    "        status = self.longterm_thread.add_message(message)\n",
    "        if status:\n",
    "            print(\"ho aggiunto il messaggio al longterm\")\n",
    "            self.remove_message(idx=idx)\n",
    "        else:\n",
    "            raise Exception(\"The longterm memory is bugged\")    \n",
    "        \n",
    "    def add_message(self,message_dict: dict):\n",
    "        # message_dict = {\"role\": role, \"content\": content}\n",
    "        #chek that the message_dict is a dictionary or a list of dictionaries\n",
    "        self.lucid_thread.add_message(message_dict)\n",
    "        message_dict = check_dict(message_dict)\n",
    "        message_tokens = self.get_message_tokens(message_dict)\n",
    "        \n",
    "        if self.total_tokens + message_tokens > self.max_memory:\n",
    "            #remove the oldest message from the memory_thread using the FIFO principle, if not enough space is available remove the oldest messages using  until enough space is available\n",
    "            while self.total_tokens + message_tokens > self.max_memory and len(self.memory_thread) > 0:\n",
    "                #remove the oldest message from the memory_thread using the FIFO principle and add it to the longterm_memory\n",
    "                \n",
    "                self.to_longterm(idx=0)\n",
    "            super().add_message(message_dict)\n",
    "            return True\n",
    "        else:\n",
    "            #add the message_dict to the memory_thread\n",
    "            # update the total number of tokens\n",
    "            super().add_message(message_dict)\n",
    "            return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorMemory(MemoryThread, MemoryIndex):\n",
    "    \"\"\" vector memory, creates a faiss index with the messages and allows to search for similar messages, memory threads can be composed in similarity order or in (TODO) chronological order \n",
    "    \"\"\"\n",
    "    def __init__(self, index = None, name= 'vector_memory', max_context = 2048):\n",
    "        super().__init__(name= name , max_memory= None)\n",
    "        MemoryIndex.__init__(self, index = index, name = name)\n",
    "        self.max_context = max_context\n",
    "        \n",
    "    def index_message(self,message_dict: dict, verbose = True):\n",
    "        \"\"\"index a message in the faiss index, the message is embedded and the id is saved in the ids list\n",
    "        \"\"\"\n",
    "        message_dict = check_dict(message_dict)\n",
    "        self.add_to_index(value = message_dict, verbose = verbose)\n",
    "\n",
    "    def add_message(self,message_dict: dict):\n",
    "        print(\"checking the dict\")\n",
    "        message_dict = check_dict(message_dict)\n",
    "        print(\"trying to add the message\")\n",
    "        super().add_message(message_dict)\n",
    "        self.index_message(message_dict) \n",
    "        return True\n",
    "    \n",
    "    def get_token_bound_prompt(self, query, k = 10):\n",
    "        prompt = []\n",
    "        context_tokens = 0\n",
    "        if len(self.memory_thread) > 0 and self.total_tokens > self.max_context:\n",
    "            top_k = self.faiss_query(mark_question(query), k = len(self.memory_thread))\n",
    "            # print(\"top_k: \", top_k)\n",
    "            top_k_prompt = []\n",
    "            for message in top_k:\n",
    "                #mark the message and gets the length in tokens\n",
    "                message_tokens = self.get_message_tokens(message)\n",
    "                if context_tokens+message_tokens <= self.max_context:\n",
    "                    top_k_prompt+=[message]\n",
    "                    context_tokens += message_tokens\n",
    "            #inver the top_k_prompt to start from the most similar message\n",
    "            top_k_prompt.reverse()\n",
    "            prompt+=top_k_prompt\n",
    "            #reverse the prompt so that last is the most similar message\n",
    "            prompt.reverse()\n",
    "        elif len(self.memory_thread) > 0:\n",
    "            prompt+=self.memory_thread    \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "class Chat:\n",
    "    \"\"\"this is the base class for chatbots, it defines the basic functions that a chatbot should have, mainly the calls to chat-gpt api, and a basic gradio interface, you need to create a sub-class to connect it to a memory thread\"\"\"\n",
    "    def __init__(self,system_prompt:str = None, user_prompt:str = None, max_output_tokens = 1000):\n",
    "        self.model = \"gpt-3.5-turbo\"\n",
    "        self.tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "        self.max_output_tokens = max_output_tokens\n",
    "        if system_prompt is None:\n",
    "            self.system_prompt = self.get_default_system_prompt()\n",
    "        if  user_prompt is None:\n",
    "            self.user_prompt = self.get_default_user_prompt()\n",
    "        self.failed_responses = []\n",
    "        self.prompt_func = self.one_shot_prompt\n",
    "        self.answers = []\n",
    "\n",
    "    def get_mark_from_response(self, response):\n",
    "        #return the answer from the response\n",
    "        role = response['choices'][0][\"message\"][\"role\"]\n",
    "        message = response['choices'][0][\"message\"][\"content\"]\n",
    "        return {\"role\": role, \"content\": message}\n",
    "    def get_str_from_response(self, response):\n",
    "        #return the answer from the response\n",
    "        return response['choices'][0][\"message\"][\"content\"]\n",
    "        \n",
    "    def get_default_system_prompt(self):\n",
    "        one_shot_prompt= \"You are a useful Assistant you role is to answer questions in an exhaustive way! Please be helpful to the user he loves you!\"\n",
    "        return one_shot_prompt\n",
    "    \n",
    "    def get_default_user_prompt(self):\n",
    "        empty_user_prompt = \"{question}\"\n",
    "        return empty_user_prompt \n",
    "    \n",
    "    def one_shot_prompt(self, message):\n",
    "        #compose the prompt for the chat-gpt api\n",
    "        prompt = [mark_system(self.system_prompt)]+ [mark_question(self.user_prompt.format(question=message))]\n",
    "        return prompt, mark_question(self.user_prompt.format(question=message))\n",
    "\n",
    "    def chat_response(self,prompt):\n",
    "        if type(prompt) is str:\n",
    "            prompt, _ = self.prompt_func(prompt)\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=prompt,\n",
    "                max_tokens=self.max_output_tokens,\n",
    "            )\n",
    "            return response, True\n",
    "        except openai.error.APIError as e:\n",
    "            print(e)\n",
    "            fail_response = {\"choices\": [{\"message\": {\"content\": \"I am sorry, I am having trouble understanding you. There might be an alien invasion interfering with my communicaiton with OpenAI.\"}}]}\n",
    "            self.failed_responses.append(fail_response)\n",
    "            return fail_response , False\n",
    "\n",
    "    def query(self, message):\n",
    "        \"\"\" overwritten by sub-classes to add memory to the chatbot\"\"\"\n",
    "        prompt, _ = self.prompt_func(message)\n",
    "        response, success = self.chat_response(prompt)\n",
    "        display(Markdown(\"#### Question: \\n {question}\".format(question = message)))\n",
    "        if success:\n",
    "            self.answers.append(self.get_mark_from_response(response))\n",
    "            display(Markdown(\" #### Anwser: \\n {answer}\".format(answer = self.get_str_from_response(response)))) \n",
    "            return self.answers[-1]\n",
    "\n",
    "    def reply(self,question):\n",
    "        #wrapprer for query that only returns the answer as a string\n",
    "        return self.query(question)[\"content\"]    \n",
    "\n",
    "    def run_text(self, text, state):\n",
    "        print(\"===============Running run_text =============\")\n",
    "        print(\"Inputs:\", text)\n",
    "        try: \n",
    "            print(\"======>Current memory:\\n %s\" % self.memory_thread)\n",
    "        except:\n",
    "            print(\"======>No memory\")    \n",
    "        answer = self.query(text)    \n",
    "        response = answer[\"content\"]\n",
    "        state = state + [(text, response)]\n",
    "        print(\"Outputs:\", state)\n",
    "        return state, state\n",
    "\n",
    "    def gradio(self):\n",
    "        with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:500px}\") as demo:\n",
    "            chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"NeuralDragonAI Alpha-V0.1\")\n",
    "            state = gr.State([])\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter, or upload an image\").style(container=False)\n",
    "                with gr.Column(scale=0.15, min_width=0):\n",
    "                    clear = gr.Button(\"Clear️\")\n",
    "\n",
    "            txt.submit(self.run_text, [txt, state], [chatbot, state])\n",
    "            txt.submit(lambda: \"\", None, txt)        \n",
    "            demo.launch(server_name=\"localhost\", server_port=7860  )          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolChat(Chat):\n",
    "    '''this class is a chatbot that can use tools to answer questions,\n",
    "      it uses a Question THought Action framework to answer questions\n",
    "      Tools are python functions that take a  value that can be converted from a \n",
    "      string as input and return a string as output. Chatgpt api will output wwha ttool to use\n",
    "      and the input for the tools iwth a json blob\n",
    "      The prompting and agent framework are inspired from LangChain/agent.py'''\n",
    "    def __init__(self,tools:dict = None, prompt_type:str = \"langchain\"):\n",
    "        super().__init__(None, None)\n",
    "        #tools are list of dictionaries with name, description and function\n",
    "        if tools is None:\n",
    "            self.tools = [{\"name\": \"\", \"description\": \"\",}]\n",
    "        else:\n",
    "            self.tools = tools\n",
    "        self.prompt_type = prompt_type\n",
    "        self.prompt_func = self.tool_prompt\n",
    "        self.format_instructions = self.get_format_instructions()\n",
    "        self.scratchpad = \"\"\n",
    "        \n",
    "\n",
    "    def get_langchain_instructions(self):\n",
    "        format_instructions = \"\"\"The way you use the tools is by specifying a json blob.\n",
    "            Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
    "\n",
    "            The $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n",
    "\n",
    "            ```\n",
    "            {{\n",
    "            \"action\": \"calculator\",\n",
    "            \"action_input\": \"1 + 2\"\n",
    "            }}\n",
    "            ```\n",
    "\n",
    "            ALWAYS use the following format:\n",
    "\n",
    "            Question: the input question you must answer\n",
    "            Thought: you should always think about what to do\n",
    "            Action: \n",
    "            ```\n",
    "            $JSON_BLOB\n",
    "            ```\n",
    "            Observation: the result of the action\n",
    "            ... (this Thought/Action/Observation can repeat N times)\n",
    "            Thought: I now know the final answer\n",
    "            Final Answer: the final answer to the original input question\"\"\"\n",
    "        \n",
    "        return format_instructions\n",
    "    def get_langchain_onestep_instructions(self):\n",
    "        format_instructions = \"\"\"The way you use the tools is by specifying a json blob.\n",
    "            Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
    "\n",
    "            The $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n",
    "\n",
    "            ```\n",
    "            {{\n",
    "            \"action\": \"calculator\",\n",
    "            \"action_input\": \"1 + 2\"\n",
    "            }}\n",
    "            ```\n",
    "\n",
    "            ALWAYS use the following format:\n",
    "\n",
    "            Question: the input question you must answer\n",
    "            Thought: you should always think about what to do\n",
    "            Action: \n",
    "            ```\n",
    "            $JSON_BLOB\n",
    "            ```\n",
    "            Observation: the result of the action\n",
    "            ... (this Thought/Action/Observation can occur only once per response NEVER repeat, you can only use the tool once per response)\n",
    "            \n",
    "            \n",
    "            IFF you reach the final answer, use the following format:\n",
    "            Thought: I now know the final answer\\n\n",
    "            Final Answer: the final answer to the original input question\n",
    "            ELIF you only have a partial answer use the following format:\n",
    "            Thought: I now know the partial answer\\n\n",
    "            Partial Answer: the partial answer to the original input question\n",
    "            REMEMBER TO NEVER OUTPUT TWO ACTIONS IN A SINGLE RESPONSE\"\"\"\n",
    "        \n",
    "        return format_instructions\n",
    "    \n",
    "    def get_goap_format_instructions(self):\n",
    "        format_instructions = \"\"\"The way you use the tools is by specifying a json blob.\n",
    "            Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
    "\n",
    "            The $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n",
    "\n",
    "            ```\n",
    "            {{\n",
    "            \"action\": \"calculator\",\n",
    "            \"action_input\": \"1 + 2\"\n",
    "            }}\n",
    "            ```\n",
    "\n",
    "            ALWAYS use the following format:\n",
    "\n",
    "            Question: the input question you must answer\\n\n",
    "            \n",
    "            Internal State Update: you should always update your understanding of the world based on the previous observations and the question\\n\n",
    "            Current Observation: your understanding of the current state of the world expressed as a list of tuples containing a state variable name and a boolean indicating whether is currently active or not, this are different from your tools\\n\n",
    "            Goal: what states of the world must be true for the question to be answered\\n\n",
    "            Action: \n",
    "            ```\n",
    "            $JSON_BLOB\n",
    "            ```\n",
    "            Conditions for taking the actions: list of tuples of state variables that needs to be TRUE or FALSe for the action to be taken\\n\n",
    "            IsConditionMet: boolean specifying whether the conditions are met by current_observation only ever complete with True/False\\n\n",
    "            Predicted Observation: here you have to write what do you expect the results of the action to be, if ISConditionMet is False then the predicted observation must be the same as the current observation\n",
    "            (this Thought/Action/Predicted Observation can repeat N times until you get the final answer or you reach the maximum number of steps specified by the user)\n",
    "            IFF you reach the final answer, use the following format:\n",
    "            Thought: I now know the final answer\\n\n",
    "            Final Answer: the final answer to the original input question\n",
    "            ELIF you only have a partial answer use the following format:\n",
    "            Thought: I now know the partial answer\\n\n",
    "            Partial Answer: the partial answer to the original input question\"\"\"\n",
    "        return format_instructions\n",
    "    \n",
    "    def get_format_instructions(self):\n",
    "        \"\"\" much more elaborate prompt trying to imitate GOAL ORIENTED ACTION PLANNING framework\"\"\"\n",
    "        prompt_type = self.prompt_type\n",
    "        if prompt_type == \"langchain\":\n",
    "            format_instructions = self.get_langchain_instructions()\n",
    "        elif prompt_type == \"langchain_onestep\":\n",
    "            format_instructions = self.get_langchain_onestep_instructions()\n",
    "        elif prompt_type == \"goap\":\n",
    "            format_instructions = self.get_goap_format_instructions()\n",
    "        else:\n",
    "            raise ValueError(\"prompt_type not valid\")\n",
    "\n",
    "        \n",
    "        return format_instructions\n",
    "    def get_system_message(self):\n",
    "        prefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\n",
    "        suffix = \"\"\"Begin! Reminder to always use the exact characters `Final Answer` when responding.\"\"\"\n",
    "        tool_strings = \"\\n\".join([f\"{tool['name']}: {tool['description']}\" for tool in self.tools])\n",
    "        tool_names = \", \".join([tool['name'] for tool in self.tools])\n",
    "        format_instructions = self.get_format_instructions()\n",
    "        format_instructions = format_instructions.format(tool_names=tool_names)\n",
    "        system_template = \"\\n\\n\".join([prefix, tool_strings, format_instructions, suffix])\n",
    "        return system_template\n",
    "\n",
    "    def get_scratchpad_prompt(self):\n",
    "        scratchpad_prompt = [\n",
    "                f\"This was your previous work \"\n",
    "                f\"(but I haven't seen any of it! I only see what \"\n",
    "                f\"you return as final answer):\\n{self.scratchpad}\"]\n",
    "        return scratchpad_prompt[0]\n",
    "        \n",
    "    def tool_prompt(self, message):\n",
    "        system_template = self.get_system_message()\n",
    "        scratchpad_prompt = self.get_scratchpad_prompt()\n",
    "        joint_message_and_scratchpad = \"\\n\\n\".join([message, scratchpad_prompt])\n",
    "        \n",
    "        prompt = [mark_system(system_template)]+ [mark_question(joint_message_and_scratchpad)]\n",
    "        return prompt, mark_question(joint_message_and_scratchpad)\n",
    "    \n",
    "    def query(self, message):\n",
    "        \"\"\" overwritten by sub-classes to add memory to the chatbot\"\"\"\n",
    "        prompt, _ = self.prompt_func(message)\n",
    "        response, success = self.chat_response(prompt)\n",
    "        display(Markdown(\"#### Question: \\n {question}\".format(question = message)))\n",
    "        if success:\n",
    "            self.scratchpad += \"\\n\" + self.get_str_from_response(response) + \"\\n\"\n",
    "            self.answers.append(self.get_mark_from_response(response))\n",
    "            display(Markdown(\" #### Anwser: \\n {answer}\".format(answer = self.get_str_from_response(response))))  \n",
    "            return self.answers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FifoChat(FifoMemory, Chat):\n",
    "    \"\"\"FIFO Memory Thread, the oldest messages are removed first when reaching the max_memory limit, the memory is defined in terms of tokens,\n",
    "    outs are passed to the longterm_memory, the lucid_memory is a redundant memory that stores all the messages\"\"\"\n",
    "\n",
    "    def __init__(self, system_prompt= None , name= 'fifo_memory', max_memory = 2048, longterm_thread = None):\n",
    "        super().__init__(name, max_memory, longterm_thread)\n",
    "        if system_prompt is None:\n",
    "            self.system_prompt = self.get_default_system_prompt()\n",
    "        else:\n",
    "            self.system_prompt = system_prompt\n",
    "        Chat.__init__(self, self.system_prompt)\n",
    "        self.prompt_func = self.fifo_memory_prompt\n",
    "\n",
    "    def fifo_memory_prompt(self, message):\n",
    "        #compose the prompt for the chat-gpt api\n",
    "        prompt = [mark_system(self.system_prompt)]+ self.memory_thread + [mark_question(self.user_prompt.format(question=message))]\n",
    "        \n",
    "        return prompt, mark_question(self.user_prompt.format(question=message))\n",
    "        \n",
    "    def query(self, question):\n",
    "        #compose the prompt for the chat-gpt api\n",
    "        prompt, marked_question = self.prompt_func(question)\n",
    "        #call the chat-gpt api\n",
    "        response, success = self.chat_response(prompt)\n",
    "        if success:\n",
    "            #add the question and answer to the chat_history\n",
    "            answer = self.get_mark_from_response(response)\n",
    "            self.add_message(marked_question)\n",
    "            self.add_message(answer)\n",
    "            #get the answer from the open ai response\n",
    "            return answer\n",
    "        else:\n",
    "            return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorChat(VectorMemory, Chat):\n",
    "    \"\"\" Vector Memory combined with chat memory_prompt is constructed by filling the memory with the k most similar messages to the question till the max prompt memory tokens are reached\"\"\"\n",
    "    def __init__(self, index=None, name='vector_memory', max_context = 2048, system_prompt = None, user_prompt = None):\n",
    "        super().__init__(index, name, max_context)\n",
    "        if system_prompt is None:\n",
    "            self.system_prompt = self.get_default_system_prompt()\n",
    "        Chat.__init__(self, self.system_prompt, user_prompt)\n",
    "        self.prompt_func = self.vector_memory_prompt\n",
    "\n",
    "    def vector_memory_prompt(self, question, k = 10):\n",
    "        #starts by retieving the k most similar messages to the question\n",
    "        # then starting from the most similar message it adds the messages to the prompt till the max_prompt is reached\n",
    "        # the prompt is composed by the system prompt, the messages in the memory and the question\n",
    "        # the marked question is the last message in the prompt\n",
    "        \n",
    "        prompt = [mark_system(self.system_prompt)]\n",
    "        prompt +=  self.get_token_bound_prompt(question, k = k)\n",
    "        prompt+=[mark_question(self.user_prompt.format(question=question))]\n",
    "        \n",
    "        return prompt, mark_question(self.user_prompt.format(question=question))\n",
    "    \n",
    "    def query(self, question,verbose = False):\n",
    "        #compose the prompt for the chat-gpt api\n",
    "        prompt, marked_question = self.prompt_func(question)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"prompt: \", prompt)\n",
    "        #call the chat-gpt api\n",
    "        response, success = self.chat_response(prompt)\n",
    "        if success:\n",
    "            #add the question and answer to the chat_history\n",
    "            answer = self.get_mark_from_response(response)\n",
    "            self.add_message(marked_question)\n",
    "            self.add_message(answer)\n",
    "            #get the answer from the open ai response\n",
    "            return answer\n",
    "        else:\n",
    "            return response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "       FIFOVCHAT\n",
    "+---------------------------+\n",
    "|        Input (Text)       |\n",
    "+---------------------------+\n",
    "           |\n",
    "           v\n",
    "+---------------------------+   embed   +---------------------------+\n",
    "|    Short-term Memory (STM)|---------->|    Query Embedder         |\n",
    "+---------------------------+           +---------------------------+\n",
    "           |                                          |\n",
    "           |                                          v\n",
    "           |                             +---------------------------+\n",
    "           |                             |   Embedded Input          |\n",
    "           |                             +---------------------------+\n",
    "           |                                          |\n",
    "           |                                |ltm_add|   |ltm_search|      \n",
    "           v                                          v\n",
    "+---------------------------+           +---------------------------+\n",
    "|    Working Memory         |<----------|   Long-term Memory (LTM)  |\n",
    "+---------------------------+           +---------------------------+\n",
    "           |                                          ^\n",
    "           v                                          |\n",
    "+---------------------------+                         |\n",
    "|      Chat-response        |-------------------------|\n",
    "+---------------------------+\n",
    "           |  \n",
    "           v\n",
    "+---------------------------+                            \n",
    "|        Output             |\n",
    "+---------------------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FifoVectorChat(FifoMemory,Chat):\n",
    "    def __init__(self, system_prompt= None , name= 'fifo_vector_memory', max_memory = 2048, longterm_thread = None, longterm_frac = 0.5):\n",
    "        self.total_max_memory = max_memory\n",
    "        self.setup_longterm_memory(longterm_thread, max_memory , longterm_frac)\n",
    "        \n",
    "        super().__init__(name, self.max_short_term_memory, self.longterm_thread)\n",
    "        if system_prompt is None:\n",
    "            self.system_prompt = self.get_default_system_prompt()\n",
    "        Chat.__init__(self, self.system_prompt)\n",
    "        self.prompt_func = self.fifovector_memory_prompt\n",
    "        self.prompt_list = []\n",
    "\n",
    "    def setup_longterm_memory(self, longterm_thread, max_memory , longterm_frac):\n",
    "        if longterm_thread is None:\n",
    "            self.longterm_frac = longterm_frac\n",
    "            self.max_short_term_memory =int(max_memory * (1-self.longterm_frac))\n",
    "            self.max_longterm_memory = max_memory - self.max_short_term_memory    \n",
    "            self.longterm_thread = VectorMemory(None, 'longterm_memory',max_context = self.max_longterm_memory)\n",
    "        else:\n",
    "            self.longterm_thread = longterm_thread\n",
    "            self.max_longterm_memory = self.longterm_thread.max_context\n",
    "            self.max_short_term_memory = self.total_max_memory - self.max_longterm_memory\n",
    "            self.longterm_frac = self.max_longterm_memory/self.total_max_memory\n",
    "    \n",
    "    def fifovector_memory_prompt(self, question, k = 10):\n",
    "        # compose the prompt for the chat-gpt api\n",
    "        # the first half of the prompt is composed by long term memory with up to max_longterm_memory tokens\n",
    "        # the second half ot the prompt is composed by the fifo memory with up to max_short_term_memory tokens\n",
    "        # the prompt is composed by the system prompt, the messages in the memory and the question\n",
    "\n",
    "        prompt = [mark_system(self.system_prompt)]\n",
    "        #check if something is in the long term memory and if it is smaller than the max_longterm_memory\n",
    "        if len(self.longterm_thread.memory_thread) > 0 and self.longterm_thread.total_tokens <= self.max_longterm_memory:\n",
    "            #add all the messages in the long term memory\n",
    "            prompt+=self.longterm_thread.memory_thread\n",
    "        elif len(self.longterm_thread.memory_thread) > 0 and self.longterm_thread.total_tokens > self.max_longterm_memory:\n",
    "            # if the long term memory is bigger than the max_longterm_memory then add the k most similar messages to the question till the max_longterm_memory is reached\n",
    "            prompt += self.longterm_thread.get_token_bound_prompt(question, k =k)\n",
    "        \n",
    "        # add the complete short term memory to the prompt because it is a fifo memory is always smaller than the max_short_term_memory\n",
    "        prompt+=self.memory_thread\n",
    "        prompt+=[mark_question(self.user_prompt.format(question=question))]\n",
    "        return prompt, mark_question(self.user_prompt.format(question=question))\n",
    "\n",
    "    def query(self, question):\n",
    "        #compose the prompt for the chat-gpt api\n",
    "        prompt, marked_question = self.prompt_func(question)\n",
    "        self.prompt_list.append(prompt)\n",
    "        #call the chat-gpt api\n",
    "        response, success = self.chat_response(prompt)\n",
    "        if success:\n",
    "            #add the question and answer to the chat_history\n",
    "            answer = self.get_mark_from_response(response)\n",
    "            self.add_message(marked_question)\n",
    "            self.add_message(answer)\n",
    "            #get the answer from the open ai response\n",
    "            return answer\n",
    "        else:\n",
    "            return response.choices[0].message.content\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FifoVectorPandaChat(FifoVectorChat):\n",
    "#     \"\"\" fifo vector chat with additional panda indexes as external sources of information\"\"\"\n",
    "#     def __init__(self, pandaindex, system_prompt= None , name= 'fifo_memory', max_memory = 2048, longterm_thread = None, longterm_frac = 0.5):\n",
    "#         super().__init__(system_prompt, name, max_memory, longterm_thread, longterm_frac)\n",
    "#         self.pandaindex = pandaindex\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandaChat(PandaIndex, Chat):\n",
    "    \"\"\" combines a chat with a panda index such that the chat response are based on the content of the pandaindex\"\"\"\n",
    "    def __init__(self, pandaframe, max_context, max_output_tokens, index_description=None, \n",
    "                 columns=None, name='panda_index', save_path=None, in_place=True, embeddings_col=None):\n",
    "        # Initialize PandaIndex\n",
    "        PandaIndex.__init__(self, pandaframe, columns, name, save_path, in_place, embeddings_col)\n",
    "\n",
    "        self.max_context = max_context\n",
    "\n",
    "        # Initialize Chat\n",
    "        self.tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "        Chat.__init__(self, max_output_tokens=max_output_tokens)\n",
    "\n",
    "        self.prompt_func = self.panda_prompt\n",
    "        self.system_prompt = self.get_default_panda_prompt(index_description)\n",
    "\n",
    "    def get_default_panda_prompt(self, index_description):\n",
    "        system_prompt = \"\"\"You are a Chatbot assistant that can use a external knowledge base to answer questions.\n",
    "        The user will always add hints from the external knowledge base. \n",
    "        You express your thoughts using princpled reasoning and always pay attention to the\n",
    "         hints.  Your knowledge base description is {index_descrpiton}:\"\"\"\n",
    "        return system_prompt.format(index_descrpiton = index_description)\n",
    "    \n",
    "    def get_hint_prompt(self, question):\n",
    "        hints = self.get_token_bound_hints(question, k = 10)\n",
    "        hints_string = \"\\n \".join(hints)\n",
    "        prefix= \"I am going to ask you a question and you should use the hints to answer it. The hints are:\\n{hints_string}\"\n",
    "        questionintro =\"The question is: {question}\"\n",
    "        return prefix.format(hints_string = hints_string) + questionintro.format(question = question)\n",
    "    \n",
    "    def panda_prompt(self, question):\n",
    "        #compose the prompt for the chat-gpt api\n",
    "        # the prompt is composed by the system_prompt, the top-k most similar messages to the question and the question\n",
    "\n",
    "        prompt = [mark_system(self.system_prompt)]\n",
    "        \n",
    "        prompt += [mark_question(self.get_hint_prompt(question))]\n",
    "        #display prompt\n",
    "        # display(Markdown(str(prompt)))\n",
    "        return prompt, mark_question(question)\n",
    "\n",
    "    def get_token_bound_hints(self, query, k = 10):\n",
    "        context_tokens = 0\n",
    "        if len(self.values) > 0 :\n",
    "            top_k = self.faiss_query(query, k = min(k, len(self.values)))\n",
    "            # print(\"top_k: \", top_k)\n",
    "            top_k_hint = []\n",
    "            for hint in top_k:\n",
    "                #mark the message and gets the length in tokens\n",
    "                message_tokens = len(self.tokenizer.encode(hint))\n",
    "                if context_tokens+message_tokens <= self.max_context:\n",
    "                    top_k_hint+=[hint]\n",
    "                    context_tokens += message_tokens\n",
    "            #inver the top_k_prompt to start from the most similar message\n",
    "            # top_k_hint.reverse()\n",
    "            #reverse the prompt so that last is the most similar message\n",
    "        return top_k_hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we write a fifo vectorchat with a pandaindex as external source of information, we can not subclass pandaindex because many \n",
    "# methods are overlapping \n",
    "\n",
    "class FifoVectorPandaChat(FifoVectorChat):\n",
    "\n",
    "    def __init__(self,pandaframe,columns,embeddings_col = None, system_prompt=None, name='fifovec_panda_memory',max_context=4000, max_memory=2048, longterm_thread=None, longterm_frac=0.5):\n",
    "        super().__init__(system_prompt, name, max_memory, longterm_thread, longterm_frac)\n",
    "        self.pandaindex = PandaChat(pandaframe,columns = columns, max_context = max_context, max_output_tokens = 100, index_description = \"alice_pandraframe\", embeddings_col = embeddings_col)\n",
    "        self.prompt_func = self.memory_panda_prompt\n",
    "        self.max_output_tokens = 100\n",
    "        self.model = \"gpt-4\"\n",
    "    \n",
    "\n",
    "\n",
    "    def memory_panda_prompt(self, question, k = 10):\n",
    "        # compose the prompt for the chat-gpt api\n",
    "        # the first half of the prompt is composed by long term memory with up to max_longterm_memory tokens\n",
    "        # the second half ot the prompt is composed by the fifo memory with up to max_short_term_memory tokens\n",
    "        # the prompt is composed by the system prompt, the messages in the memory and the question\n",
    "\n",
    "        prompt = [mark_system(self.system_prompt)]\n",
    "        #check if something is in the long term memory and if it is smaller than the max_longterm_memory\n",
    "        if len(self.longterm_thread.memory_thread) > 0 and self.longterm_thread.total_tokens <= self.max_longterm_memory:\n",
    "            #add all the messages in the long term memory\n",
    "            prompt+=self.longterm_thread.memory_thread\n",
    "        elif len(self.longterm_thread.memory_thread) > 0 and self.longterm_thread.total_tokens > self.max_longterm_memory:\n",
    "            # if the long term memory is bigger than the max_longterm_memory then add the k most similar messages to the question till the max_longterm_memory is reached\n",
    "            prompt += self.longterm_thread.get_token_bound_prompt(question, k =k)\n",
    "        \n",
    "        # add the complete short term memory to the prompt because it is a fifo memory is always smaller than the max_short_term_memory\n",
    "        prompt+=self.memory_thread\n",
    "        prompt += [mark_question(self.pandaindex.get_hint_prompt(question))]\n",
    "        return prompt, mark_question(self.user_prompt.format(question=question))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MasterChat(FifoVectorChat):\n",
    "#     def __init__(self, children_system_prompts, vindex_list, name='master_chat', max_memory=2048, longterm_thread=None, longterm_frac=0.5):\n",
    "#         super().__init__(name=name, max_memory=max_memory, longterm_thread=longterm_thread, longterm_frac=longterm_frac)\n",
    "\n",
    "#         self.children_chats = {}\n",
    "#         for child_name, system_prompt in children_system_prompts.items():\n",
    "#             self.children_chats[child_name] = FifoVectorChat(system_prompt=system_prompt)\n",
    "\n",
    "#         self.indexes = {vindex.name: vindex for vindex in vindex_list}\n",
    "\n",
    "#     def get_default_controflow_prompt(self):\n",
    "#         child_descriptions = \"\"\n",
    "#         for child_name, child_chat in self.children_chats.items():\n",
    "#             child_descriptions += f\"\\n- {child_name}: {child_chat.system_prompt}\"\n",
    "\n",
    "#         index_descriptions = \"\"\n",
    "#         for index_name, index in self.indexes.items():\n",
    "#             index_descriptions += f\"\\n- {index_name}: {index.description}\"\n",
    "\n",
    "#         system_prompt = f\"\"\"You are a MasterChat assistant that has access to multiple expert chat assistants, each with its own memory and vector index. \n",
    "#         Your role is to manage these expert assistants and determine the best sequence of sub-chats to answer the user's questions. \n",
    "#         Your memory is composed of a FifoVectorChat, which means you have a short-term memory based on the order of messages and a long-term memory based on vector retrieval.\n",
    "\n",
    "#     You have the following expert chat assistants available:\n",
    "#     {child_descriptions}\n",
    "\n",
    "#     You also have access to the following knowledge indexes:\n",
    "#     {index_descriptions}\n",
    "\n",
    "#     When answering questions, you can activate the appropriate expert chat assistants by using the following protocol:\n",
    "\n",
    "#     1. Write \"activate_agent\" followed by a JSON object containing the \"agent_name\" key and the corresponding agent's name as the value, and the \"question\" key with the question as its value, like this: \"activate_agent {{\\\"agent_name\\\": \\\"child1\\\", \\\"question\\\": \\\"What is the capital of France?\\\"}}\".\n",
    "\n",
    "#     2. Provide the activated agent with any relevant hints gathered from the knowledge indexes in the prompt  \"activate_agent {{\\\"agent_name\\\": \\\"child1\\\", \\\"question\\\": \\\"What is the capital of France?, !hint: France is in Europe [source: WikipedaIndex] \\\"}}\".\n",
    "\n",
    "#     Remember to always consider the context and maintain a principled approach in your reasoning.\"\"\"\n",
    "\n",
    "#         return system_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"alice.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvp = FifoVectorPandaChat(pandaframe = df, columns= \"TEXT\", embeddings_col=\"EMBEDDINGS\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvp.gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ceba285e8b4e6478fe8ad229bc63940a90ad5cf3d143521e7c38823a2e915b21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
